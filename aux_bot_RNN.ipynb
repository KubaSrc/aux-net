{"cells":[{"cell_type":"markdown","metadata":{"id":"jTknX5mmsByn"},"source":["# Training functions"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"teZT9v9ORiMb","executionInfo":{"status":"ok","timestamp":1695050987490,"user_tz":240,"elapsed":8382,"user":{"displayName":"Kuba Kowalewski","userId":"08857268640727880536"}}},"outputs":[],"source":["%matplotlib inline\n","\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import sys\n","import os\n","import time\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","from mpl_toolkits.mplot3d import Axes3D\n","from matplotlib import pyplot as plt\n","from matplotlib.colors import Normalize\n","from google.colab import output\n","from scipy.io import savemat\n","output.enable_custom_widget_manager()\n","\n","\n","class aux_bot():\n","\n","    def __init__(self,drive_path = None, data_path = None,\n","                 train_forward = False, train_inverse = False,\n","                 normalize=True):\n","        # Setup device\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        print(\"Connected: \", self.device)\n","        self.data_path = data_path\n","        self.drive_path = drive_path\n","        self.model_path = None\n","        self.normalize = normalize\n","\n","        # Parameter indicies\n","        self.m_inputs = 5\n","        self.n_outputs = 7\n","        self.motor_slice = slice(0,self.m_inputs)\n","        self.end_slice = slice(self.m_inputs,self.m_inputs+self.n_outputs)\n","        self.end_slice_xyz = slice(self.m_inputs,self.m_inputs+3)\n","        self.input_size = self.motor_slice.stop - self.motor_slice.start\n","        self.output_size = self.end_slice.stop - self.end_slice.start\n","\n","        # Constant for forward model\n","        self.BATCH_SIZE = 16\n","        self.EPOCHS = 100\n","        self.LEARNING_RATE = 0.0005\n","        self.MOMENTUM = 0.9\n","        self.WEIGHT_DECAY = 0\n","        self.DROPOUT = 0\n","        self.LAYERS = 1\n","\n","        # Upload data as fixed length trajectories instead of single points\n","        self.SEQ_LENGTH = 32\n","        self.upload_data(drive_path+data_path,batch_size = self.BATCH_SIZE,seq_length = self.SEQ_LENGTH)\n","\n","        # Train forward model\n","        if  train_forward:\n","            self.fk_net = self.forward_net(inputs = self.input_size,\n","                                           hidden_lstm = 512,\n","                                           hidden = 1552,\n","                                           outputs = self.output_size,\n","                                           num_layers = 1,\n","                                           dropout = self.DROPOUT,\n","                                           device = self.device)\n","            # Train and evaluate\n","            self.train_forward(self.fk_net,self.EPOCHS,self.LEARNING_RATE,self.MOMENTUM,self.WEIGHT_DECAY,annealing=True)\n","            self.eval_model_forward(self.fk_net,self.fk_train_loss)\n","\n","        # Load forward model instead\n","        if not train_forward:\n","            self.fk_net = self.forward_net(hidden_lstm=512, hidden=1552)\n","            self.fk_net.load_state_dict(torch.load(drive_path + '/models/forward_2023_08_22-19_21_14_8.550mm'))\n","            self.fk_net.eval()\n","            print(\"[aux_net] Forward model succesfully loaded\")\n","\n","        # Constant for inverse model\n","        self.EPOCHS = 100\n","        self.LEARNING_RATE = 0.001\n","        self.MOMENTUM = 0.9\n","        self.DROPOUT = 0\n","        self.LAYERS = 1\n","\n","        # Train inverse model\n","        if  train_inverse:\n","            self.ik_net = self.inverse_net(hidden=1552,\n","                                           hidden_lstm = 512,\n","                                           inputs=self.output_size,\n","                                           outputs=self.input_size,\n","                                           device=self.device,\n","                                           dropout=self.DROPOUT,\n","                                           num_layers=self.LAYERS,\n","                                           linear_depth=3)\n","            # Train and evaluate\n","            self.train_inverse(self.ik_net,self.EPOCHS,self.LEARNING_RATE,self.MOMENTUM,self.WEIGHT_DECAY,annealing=True)\n","            self.eval_model_inverse(self.ik_net,self.ik_train_loss)\n","\n","        # Load forward model instead\n","        if not train_inverse:\n","            self.ik_net = self.inverse_net(hidden_lstm=512, hidden=1552,device=self.device,linear_depth=3)\n","            self.ik_net.load_state_dict(torch.load(drive_path + '/models/inverse_2023_09_13-17_35_20_4.056pwm'))\n","            self.ik_net.eval()\n","            self.ik_net.to(self.device)\n","            print(\"[aux_net] Inverse model succesfully loaded\")\n","\n","    # Apply data normalization\n","    def normalize_data(self,data,d_type=\"full\"):\n","        if d_type == \"full\":\n","            data = (data-self.min_scale)/(self.max_scale-self.min_scale)\n","        elif d_type == \"motor\":\n","                data = (data-self.min_scale[self.motor_slice])/(self.max_scale[self.motor_slice]-self.min_scale[self.motor_slice])\n","        elif d_type == \"end_full\":\n","            data = (data-self.min_scale[self.end_slice])/(self.max_scale[self.end_slice]-self.min_scale[self.end_slice])\n","        return data\n","\n","    # Return data to original units\n","    def denormalize_data(self, data, d_type=\"full\"):\n","        if d_type == \"full\":\n","            data = data*(self.max_scale-self.min_scale)+self.min_scale\n","        elif d_type == \"motor\":\n","                data = data*(self.max_scale[self.motor_slice]-self.min_scale[self.motor_slice])+self.min_scale[self.motor_slice]\n","        elif d_type == \"end_full\":\n","            data = data*(self.max_scale[self.end_slice]-self.min_scale[self.end_slice])+self.min_scale[self.end_slice]\n","        return data\n","\n","    class SequenceDataset(Dataset):\n","        def __init__(self, data_array, input_range, output_range, sequence_length=5,\n","                     shuffle=True, train=False, test=False, split=0.8,inverse=False):\n","            self.shuffle = shuffle\n","            self.sequence_length = sequence_length\n","            self.train = train\n","            self.test = test\n","            self.split = split\n","            if not inverse:\n","                self.y = torch.tensor(data_array[:, output_range]).float()\n","                self.X = torch.tensor(data_array[:, input_range]).float()\n","            else:\n","                self.y = torch.tensor(data_array[:, input_range]).float()\n","                self.X = torch.tensor(data_array[:, output_range]).float()\n","            self.N = self.X.shape[0]\n","\n","            # Data shuffling\n","            if shuffle == True:\n","                torch.manual_seed(7)\n","                self.idx = torch.randperm(self.N)\n","\n","            # Handle train-test split\n","            if self.train:\n","                self.N = int(self.N*self.split)\n","                if self.shuffle: self.idx = self.idx[0:self.N]\n","            if self.test:\n","                self.N = int(self.N*(1-self.split))\n","                if self.shuffle: self.idx = self.idx[-self.N:]\n","\n","        def __len__(self):\n","            return self.N\n","\n","        def __getitem__(self, i):\n","            assert 0 <= i < self.N, \"Index out of bounds for sequence\"\n","\n","            if self.shuffle: i = self.idx[i]\n","\n","            if i >= self.sequence_length - 1:\n","                i_start = i - self.sequence_length + 1\n","                x = self.X[i_start:(i + 1), :]\n","            else:\n","                padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n","                x = self.X[0:(i + 1), :]\n","                x = torch.cat((padding, x), 0)\n","\n","            return x, self.y[i]\n","\n","        def get_all_items(self):\n","            Xs, Ys = [], []\n","            for i in range(self.N):\n","                x, y = self.__getitem__(i)\n","                Xs.append(x)\n","                Ys.append(y)\n","            return torch.stack(Xs), torch.stack(Ys)\n","\n","    # Uploads training data\n","    def upload_data(self, data_path, trim_length = None, batch_size = 1, seq_length = 1):\n","        '''\n","        Inputs in data are motor positions [dtm_13, dtm_24, de_13, de_24, d_l]\n","        Out positions [x_top (0), y_top (1), z_top (2), qx_top (3), qy_top (4), qz_top (5), qw_top (6)]\n","        '''\n","        # Upload and convert units from m to mm\n","        self.data = np.loadtxt(data_path, skiprows = 1, delimiter=',', dtype = 'float32',\n","                               usecols = tuple(range(2, 2 + self.input_size + self.output_size)))\n","        self.data[:,self.end_slice_xyz] *= 10e2\n","        self.data_raw = np.copy(self.data)\n","\n","        print(\"Inputs: \", self.data[0:3,self.motor_slice,])\n","        print(\"Outputs: \", self.data[0:3,self.end_slice,])\n","\n","        self.min_scale = np.amin(self.data,0)\n","        self.max_scale = np.amax(self.data,0)\n","\n","        if self.normalize: self.data = self.normalize_data(self.data)\n","\n","        # FORWARD train sequences\n","        self.forward_train_data = self.SequenceDataset(self.data,\n","                                                        input_range = self.motor_slice,\n","                                                        output_range = self.end_slice,\n","                                                        sequence_length = seq_length,\n","                                                        shuffle = True,\n","                                                        train = True,\n","                                                        inverse = False)\n","\n","        # FORWARD RAW test sequences\n","        self.forward_raw_test_data = self.SequenceDataset(self.data_raw,\n","                                                        input_range = self.motor_slice,\n","                                                        output_range = self.end_slice,\n","                                                        sequence_length = seq_length,\n","                                                        shuffle = True,\n","                                                        test = True,\n","                                                        inverse = False)\n","\n","        # FORWARD test sequences\n","        self.forward_test_data = self.SequenceDataset(self.data,\n","                                                        input_range = self.motor_slice,\n","                                                        output_range = self.end_slice,\n","                                                        sequence_length = seq_length,\n","                                                        shuffle = True,\n","                                                        test = True,\n","                                                        inverse = False)\n","\n","        # Set up FORWARD data loader\n","        self.forward_train_loader = torch.utils.data.DataLoader(self.forward_train_data, batch_size = batch_size)\n","        self.forward_test_loader = torch.utils.data.DataLoader(self.forward_test_data, batch_size = batch_size)\n","\n","        # INVERSE train sequences\n","        self.inverse_train_data = self.SequenceDataset(self.data,\n","                                                        input_range = self.motor_slice,\n","                                                        output_range = self.end_slice,\n","                                                        sequence_length = seq_length,\n","                                                        shuffle = True,\n","                                                        train = True,\n","                                                        inverse = True)\n","\n","        # INVERSE RAW test sequences\n","        self.inverse_raw_test_data = self.SequenceDataset(self.data_raw,\n","                                                        input_range = self.motor_slice,\n","                                                        output_range = self.end_slice,\n","                                                        sequence_length = seq_length,\n","                                                        shuffle = True,\n","                                                        test = True,\n","                                                        inverse = True)\n","\n","        # INVERSE test sequences\n","        self.inverse_test_data = self.SequenceDataset(self.data,\n","                                                        input_range = self.motor_slice,\n","                                                        output_range = self.end_slice,\n","                                                        sequence_length = seq_length,\n","                                                        shuffle = True,\n","                                                        test = True,\n","                                                        inverse = True)\n","\n","        # Set up INVERSE data loader\n","        self.inverse_train_loader = torch.utils.data.DataLoader(self.inverse_train_data, batch_size=batch_size)\n","        self.inverse_test_loader = torch.utils.data.DataLoader(self.inverse_test_data, batch_size=batch_size)\n","\n","        # Save our FORWARD and INVERSE test data to plot performance\n","        X_test_forward, Y_test_forward = self.forward_raw_test_data.get_all_items()\n","        X_test_inverse, Y_test_inverse = self.inverse_raw_test_data.get_all_items()\n","        sub_path = self.drive_path + '/data/subset/' + self.data_path.split('/')[2]\n","        savemat(sub_path.split('.')[0] + '.mat',{'X_test_forward' : X_test_forward,\n","                                                 'Y_test_forward' : Y_test_forward,\n","                                                 'X_test_inverse' : X_test_inverse,\n","                                                 'Y_test_inverse' : Y_test_inverse})\n","\n","\n","    ########################\n","    # FORWARD NETWORK\n","    ########################\n","\n","\n","    class forward_net(nn.Module):\n","        def __init__(self, inputs=5, hidden_lstm=512, hidden=1024, outputs=7,num_layers=1,dropout=0,device=None):\n","            super().__init__()\n","            self.device = device\n","\n","            self.lstm = nn.LSTM(\n","            input_size=inputs,\n","            hidden_size=hidden_lstm,\n","            batch_first=True,\n","            num_layers=num_layers,\n","            )\n","\n","            self.fc1 = nn.Linear(2 * hidden_lstm, hidden)\n","            self.fc2 = nn.Linear(hidden, hidden)\n","            self.fc3 = nn.Linear(hidden, outputs)\n","\n","            self.input_size = inputs\n","            self.hidden_size = hidden\n","            self.hidden_lstm_size = hidden_lstm\n","            self.output_size = outputs\n","            self.num_layers = num_layers\n","            self.dropout = nn.Dropout(dropout)\n","\n","        def forward(self,x):\n","            batch_size = x.shape[0]\n","            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_lstm_size).requires_grad_().to(self.device)\n","            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_lstm_size).requires_grad_().to(self.device)\n","\n","            _, (hn, cn) = self.lstm(x, (h0, c0))\n","\n","            y = torch.cat((hn[0], cn[0]), dim=1)\n","\n","            y = F.relu(y)\n","            y = self.fc1(y)\n","            y = F.relu(y)\n","            y = self.fc2(y)\n","            y = F.relu(y)\n","            y = self.fc3(y)\n","\n","            return y\n","\n","    def quaternion_angular_error(self, q1, q2):\n","        # Determine if input is numpy or torch\n","        if isinstance(q1, np.ndarray):\n","            # For NumPy arrays\n","            dot_product = np.sum(q1 * q2, axis=1)\n","            dot_product = np.clip(dot_product, -1.0, 1.0)\n","            absolute_dot_product = np.abs(dot_product)\n","            half_angle = np.arccos(absolute_dot_product)\n","\n","        elif isinstance(q1, torch.Tensor):\n","            # For PyTorch tensors\n","            dot_product = (q1 * q2).sum(dim=1).clamp(-1.0, 1.0)\n","            absolute_dot_product = torch.abs(dot_product)\n","            half_angle = torch.acos(absolute_dot_product)\n","        else:\n","            raise TypeError(\"Unsupported input type. Expected numpy array or torch tensor.\")\n","\n","        # Double to get full angle, return in degrees\n","        return 2 * half_angle * (180.0 / np.pi)\n","\n","    # Custom loss function\n","    def custom_loss(self,y_pred, y_measured):\n","        mse = nn.MSELoss()\n","        mse_loss = mse(y_pred[:, :3], y_measured[:, :3])\n","        quaternion_loss = self.quaternion_angular_error(y_pred[:, 3:], y_measured[:, 3:]).mean()\n","        return (mse_loss + 0.5*quaternion_loss)/1.5\n","\n","    # Training function\n","    def train_forward(self,fk_net,epochs=10, lr=0.01, momentum=0.9, decay=0.0, verbose=1,annealing=True):\n","        fk_net.to(self.device)\n","        losses = []\n","        criterion = nn.MSELoss()\n","        self.optimizer = optim.Adam(fk_net.parameters(), lr, weight_decay=decay)\n","\n","        # Aneal learning rate\n","        if annealing:\n","            scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.9)\n","\n","        for epoch in range(epochs):\n","            sum_loss = 0\n","            loss_list = []\n","            i = 0\n","            for X, y in self.forward_train_loader:\n","                X, y = X.to(self.device), y.to(self.device)\n","\n","                # Reset gradients\n","                self.optimizer.zero_grad()\n","\n","                # Run network forward, backward, and then update\n","                y_pred = fk_net(X)\n","                loss = self.custom_loss(y_pred,y)\n","                loss.backward()\n","                self.optimizer.step()\n","\n","                # Track performance\n","                loss_list.append(loss.item())\n","                sum_loss += loss.item()\n","\n","                if i % 100 == 99:\n","                    if verbose:\n","                        print('(Epoch {}, Iter {}) loss: {:.6f}'.format(epoch + 1, i + 1, sum_loss / 100))\n","                    sum_loss = 0\n","                i+=1\n","            if annealing:\n","                scheduler.step()\n","            losses.append(np.mean(loss_list))\n","            loss_list = []\n","        self.fk_train_loss = losses\n","\n","\n","    # Determine model error for predicting end effector pose\n","    def test_forward(self,fk_net,data='train'):\n","        accuracy = []\n","        err = np.array([])\n","        q_err = np.array([])\n","\n","        # Create data loader\n","        if data == 'train': loader = self.forward_train_loader\n","        if data == 'test': loader = self.forward_test_loader\n","\n","        # Run through the data\n","        for X, y in loader:\n","            X, y = X.to(self.device), y.to(self.device)\n","\n","            y_pred = fk_net(X)\n","\n","            # Convert back to CPU side\n","            y_pred = y_pred.numpy(force=True)\n","            y = y.numpy(force=True)\n","\n","            if self.normalize:\n","                y_pred = self.denormalize_data(y_pred,\"end_full\")\n","                y = self.denormalize_data(y,\"end_full\")\n","\n","            # Calculate postional error\n","            e = y_pred[:,0:3]-y[:,0:3]\n","            error = np.linalg.norm(e,axis = 1).flatten()\n","            accuracy.append(np.mean(error))\n","            err = np.append(err,error)\n","\n","            # Calculate orientation error\n","            q_e = self.quaternion_angular_error(y_pred[:, 3:], y[:, 3:])\n","            q_error = q_e.flatten()\n","            q_err = np.append(q_err,q_error)\n","\n","        return np.mean(accuracy), err, q_err\n","\n","    # Plots the test accuracy of the model\n","    def eval_model_forward(self,fk_net,loss):\n","        # Plot losses during training\n","        plt.close('all')\n","        f = plt.figure()\n","        f.set_size_inches(16,4)\n","        ax1 = f.add_subplot(1,3,1)\n","        ax2 = f.add_subplot(1,3,2)\n","        ax3 = f.add_subplot(1,3,3,projection='3d')\n","\n","        # Plot training loss\n","        ax1.plot(np.log10(loss),linewidth = 2)\n","        ax1.set_xlabel(\"Epochs\",fontsize = 14)\n","        ax1.set_ylabel(\"log10(Loss)\",fontsize = 14)\n","        ax1.set_title(\"Train Loss (Forward)\",fontsize = 16)\n","        ax1.grid()\n","\n","        # Determine train and test accuracy\n","        test_accuracy,test_error, test_q_error = self.test_forward(fk_net,data='test')\n","        train_accuracy, train_error, train_q_error = self.test_forward(fk_net,data='train')\n","\n","        print(\"Train accuracy {:.10f} mm Test accuracy: {:.10f} mm\".format(train_accuracy,test_accuracy))\n","\n","        # Show testing accuracy\n","        ax2.hist(test_error,20)\n","        ax2.set_xlabel(\"Error (mm)\",fontsize = 14)\n","        ax2.set_ylabel(\"Count\",fontsize = 14)\n","        ax2.set_title(\"Positional Error for Test Data\",fontsize = 16)\n","        ax2.grid()\n","\n","        # Create a colormap and a normalize function based on the errors' range\n","        colormap = cmap=plt.cm.jet\n","        xyz_quat = np.zeros((len(self.forward_test_data),7))\n","\n","        for i in range(len(self.forward_test_data)):\n","            X,y = self.forward_test_data[i]\n","            y = y.numpy(force=True)\n","            y = self.denormalize_data(y,\"end_full\")\n","            xyz_quat[i,:] = y\n","\n","        # Create scatter plot of distance errors\n","        p = ax3.scatter(xyz_quat[:,0], xyz_quat[:,2], xyz_quat[:,1], c = test_error, cmap=plt.cm.jet,vmin=0,vmax=0.65*np.max(test_error), alpha=0.2)\n","        ax3.view_init(45, 45)\n","        cb = f.colorbar(p)\n","        cb.set_label(\"Error (mm)\")\n","        ax3.set_xlabel(\"xe\",fontsize = 14)\n","        ax3.set_ylabel(\"ye\",fontsize = 14)\n","        ax3.set_zlabel(\"ze\",fontsize = 14)\n","        ax3.set_title(\"Aux-Net Forward Model Performance\",fontsize = 16)\n","        ax3.grid()\n","        plt.tight_layout()\n","        plt.show()\n","\n","        # Save model\n","        model_name = 'forward_' + time.strftime('%Y_%m_%d-%H_%M_%S_') + '{:.3f}mm'.format(test_accuracy)\n","        torch.save(fk_net.state_dict(),self.drive_path + '/models/' + model_name)\n","\n","        savemat(self.drive_path + \"/metrics/\" + model_name,{'test_error' : test_error,\n","                                                                    'test_q_error' : test_q_error})\n","\n","        return test_accuracy\n","\n","\n","    #########################\n","    ## INVERSE NETWORK\n","    #########################\n","    class inverse_net(nn.Module):\n","        def __init__(self, inputs=7, hidden_lstm=512, hidden=1024, outputs=5, num_layers=1, dropout=0.1, device=None, linear_depth = None):\n","            super().__init__()\n","\n","            self.device = device\n","\n","            self.lstm = nn.LSTM(\n","            input_size=inputs,\n","            hidden_size=hidden_lstm,\n","            batch_first=True,\n","            num_layers=num_layers,\n","            )\n","\n","            self.drop = nn.Dropout(dropout)\n","\n","            # Default size\n","            if linear_depth == None:\n","                self.fc1 = nn.Linear(hidden_lstm,hidden)\n","                self.fc2 = nn.Linear(hidden,hidden)\n","                self.fc3 = nn.Linear(hidden,outputs)\n","\n","            # Explore different sizes\n","            else:\n","                self.linear_layers = nn.ModuleList()  # Use nn.ModuleList\n","                if linear_depth == 1:\n","                    self.linear_layers.append(nn.Linear(2 * hidden_lstm, outputs))\n","                else:\n","                    self.linear_layers.append(nn.Linear(2 * hidden_lstm, hidden))\n","                    for _ in range(1, linear_depth - 1):\n","                        self.linear_layers.append(nn.Linear(hidden, hidden))\n","                    self.linear_layers.append(nn.Linear(hidden, outputs))\n","\n","            self.input_size = inputs\n","            self.hidden_size = hidden\n","            self.hidden_lstm_size = hidden_lstm\n","            self.output_size = outputs\n","            self.num_layers = num_layers\n","            self.dropout = dropout\n","            self.linear_depth = linear_depth\n","\n","        def forward(self,y):\n","\n","            batch_size = y.shape[0]\n","            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_lstm_size).requires_grad_().to(self.device)\n","            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_lstm_size).requires_grad_().to(self.device)\n","\n","            _, (hn, cn) = self.lstm(y, (h0, c0))\n","            x = torch.cat((hn[0], cn[0]), dim=1)\n","\n","            x = self.drop(x)\n","\n","            # Default architecture\n","            if self.linear_depth == None:\n","                x = F.relu(x)\n","                x = self.fc1(x)\n","                x = F.relu(x)\n","                x = self.fc2(x)\n","                x = F.relu(x)\n","                x = self.fc3(x)\n","\n","            # n-dimensional\n","            else:\n","                for fc in self.linear_layers:\n","                    x = F.relu(x)\n","                    x = fc(x)\n","\n","            return x\n","\n","    # Training function\n","    def train_inverse(self,ik_net,epochs=10, lr=0.01, momentum=0.9, decay=0.0, verbose=1, annealing=True):\n","        ik_net.to(self.device)\n","        losses = []\n","        criterion = nn.MSELoss()\n","        self.optimizer = optim.Adam(ik_net.parameters(), lr, weight_decay=decay)\n","\n","        # Anneal the learing rate over time\n","        if annealing:\n","            scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.9)\n","\n","        for epoch in range(epochs):\n","            sum_loss = 0\n","            loss_list = []\n","            i = 0\n","            for X, y in self.inverse_train_loader:\n","                X, y = X.to(self.device), y.to(self.device)\n","\n","                # Reset gradients\n","                self.optimizer.zero_grad()\n","\n","                # Run network forward, backward, and then update\n","                y_pred = ik_net(X)\n","                loss = criterion(y_pred,y)\n","                loss.backward()\n","                self.optimizer.step()\n","\n","                # Track performance\n","                loss_list.append(loss.item())\n","                sum_loss += loss.item()\n","                if i % 100 == 99:\n","                    if verbose:\n","                        print('(Epoch {}, Iter {}) loss: {:.6f}'.format(epoch + 1, i + 1, sum_loss / 100))\n","                    sum_loss = 0\n","                i += 1\n","            if annealing:\n","                scheduler.step()\n","            losses.append(np.mean(loss_list))\n","            loss_list = []\n","        self.ik_train_loss = losses\n","\n","    # Determine model error from predicting motor inputs\n","    def test_inverse(self,ik_net,data='train'):\n","        accuracy = []\n","        err = np.array([])\n","        dist = np.array([])\n","\n","        # Create data loader\n","        if data == 'train': loader = self.inverse_train_loader\n","        if data == 'test': loader = self.inverse_test_loader\n","\n","        # Run through the data\n","        for X, y in loader:\n","            X, y = X.to(self.device), y.to(self.device)\n","\n","            # Run network forward, backward, and then update\n","            y_pred = ik_net(X)\n","\n","            # Denormalize data to find error\n","            if self.normalize:\n","                y_pred = self.denormalize_data(y_pred.numpy(force=True),\"motor\")\n","                y = self.denormalize_data(y.numpy(force=True),\"motor\")\n","                X = self.denormalize_data(X.numpy(force=True),\"end_full\")\n","\n","            # Calculate error\n","            e = y_pred-y\n","            error = np.linalg.norm(e,axis = 1).flatten()\n","            accuracy.append(np.mean(error))\n","            err = np.append(err,error)\n","\n","            # Record distance of this trajectory\n","            X = X[:,:,:3]\n","            diffs = np.diff(X, axis=1)  # This will reduce the 2nd dimension by 1\n","            point_dist = np.linalg.norm(diffs, axis=-1)\n","            seq_dis = np.sum(point_dist, axis=1)\n","            dist = np.append(dist, seq_dis)\n","\n","        return np.mean(accuracy), err, dist\n","\n","    # Plots the test accuracy of the model\n","    def eval_model_inverse(self,ik_net,loss):\n","        # Plot losses during training\n","        f = plt.figure()\n","        f.set_size_inches(20,5)\n","        ax1 = f.add_subplot(1,3,1)\n","        ax2 = f.add_subplot(1,3,2)\n","        ax3 = f.add_subplot(1,3,3,projection='3d')\n","\n","        # Plot training loss\n","        ax1.plot(np.log10(loss),linewidth = 2)\n","        ax1.set_xlabel(\"Epochs\",fontsize = 14)\n","        ax1.set_ylabel(\"log10(Loss)\",fontsize = 14)\n","        ax1.set_title(\"Train Loss (Inverse)\",fontsize = 16)\n","        ax1.grid()\n","\n","        # Determine train and test accuracy\n","        test_accuracy,test_error,test_dist = self.test_inverse(ik_net,data='test')\n","        train_accuracy, train_error,train_dist = self.test_inverse(ik_net,data='train')\n","        self.test_accuracy = test_accuracy\n","\n","        print(\"Train accuracy {:.10f} pwm Test accuracy: {:.10f} pwm\".format(train_accuracy,test_accuracy))\n","\n","        # Show testing accuracy\n","        ax2.hist(test_error,int(np.sqrt(len(test_error))))\n","        ax2.set_xlabel(\"Error (pwm)\",fontsize = 14)\n","        ax2.set_ylabel(\"Count\",fontsize = 14)\n","        ax2.set_title(\"PWM Error for Test Data\",fontsize = 16)\n","        ax2.set_xlim(0,25)\n","        ax2.grid()\n","\n","        # Create a colormap and a normalize function based on the errors' range\n","        colormap = cmap=plt.cm.jet\n","        norm = Normalize(vmin=test_error.min(), vmax=0.6*test_error.max())\n","\n","        for i in range(len(self.inverse_test_data)):\n","            X,y = self.inverse_test_data[i]\n","            X = X.numpy(force=True)\n","            X = self.denormalize_data(X,\"end_full\")\n","            color = colormap(norm(test_error[i]))\n","            plt.plot(X[:,0],X[:,2],X[:,1],marker='o', color=color, alpha=0.2)\n","\n","        # Plot formatting\n","        ax3.view_init(45, 45)\n","        cb = f.colorbar(plt.cm.ScalarMappable(cmap=colormap, norm=norm), ax=ax3, shrink=0.6)\n","        cb.set_label(\"Error (pwm)\")\n","        ax3.set_xlabel(\"xe\",fontsize = 14)\n","        ax3.set_ylabel(\"ye\",fontsize = 14)\n","        ax3.set_zlabel(\"ze\",fontsize = 14)\n","        ax3.set_title(\"Aux-Net Inverse Model Performance\",fontsize = 16)\n","        ax3.grid()\n","        plt.tight_layout()\n","        plt.show()\n","\n","        # Save model\n","        torch.save(ik_net.state_dict(),self.drive_path +'/models/inverse_'+time.strftime('%Y_%m_%d-%H_%M_%S_') + '{:.3f}pwm'.format(test_accuracy))\n","\n","        f = plt.figure()\n","        plt.scatter(train_dist,train_error)\n","        plt.scatter(test_dist,test_error)\n","        plt.legend([\"test\",\"train\"])\n","        plt.xlabel(\"trajectory distance (mm)\",fontsize = 14)\n","        plt.ylabel(\"inverse model error (pwm)\",fontsize = 14)\n","        plt.show()\n","\n","        return test_accuracy\n","\n","    # Train and validation for Bayesian optimization of model\n","    def train_validate(self, lr: float, hidden_lstm: float, hidden: float):\n","\n","            hidden_lstm = int(hidden_lstm)\n","            hidden = int(hidden)\n","\n","            self.optima_net = self.inverse_net(hidden=hidden,\n","                                    hidden_lstm = hidden_lstm,\n","                                    inputs=self.output_size,\n","                                    outputs=self.input_size,\n","                                    device=self.device,\n","                                    dropout=self.DROPOUT,\n","                                    num_layers=1)\n","\n","            # Train and evaluate\n","            self.train_inverse(self.optima_net,self.EPOCHS,lr,self.MOMENTUM,self.WEIGHT_DECAY,annealing=True,verbose=0)\n","            validation,_,_ = self.test_inverse(self.optima_net,data=\"test\")\n","\n","            return -validation\n","\n","    # Run inverse prediction\n","    def inverse_prediction(self,input_path=None,output_path=None):\n","        with torch.no_grad():\n","            # Find end effector position we want to predict for inverse verification\n","            if input_path == None:\n","\n","                n_points = 7500\n","\n","                prediction_data = self.SequenceDataset(self.data[0:n_points,:],\n","                                                       input_range = self.end_slice,\n","                                                       output_range = self.motor_slice,\n","                                                       sequence_length = self.SEQ_LENGTH,\n","                                                       shuffle = False,\n","                                                       train = False,\n","                                                       inverse = False)\n","                # Gather all sequence data\n","                X, y = prediction_data.get_all_items()\n","                Y_pred = self.ik_net(X.to(self.device))\n","\n","                # Create numpy arrays\n","                Y_pred = Y_pred.cpu()\n","                Y_pred = self.denormalize_data(Y_pred.numpy(),\"motor\")\n","                y = self.denormalize_data(y.numpy(),\"motor\")\n","\n","                print(\"Predicted: \", Y_pred[0:10,:])\n","                print(\"Measured: \", y[0:10,:])\n","\n","                np.savetxt(self.drive_path+\"/input_regression_7500.csv\",Y_pred,delimiter=\",\",header=\"dtm_13,dtm_24,de_13,de_24,dl\",comments='')\n","\n","            # Generate motor trajectory\n","            else:\n","                traj = torch.from_numpy(np.loadtxt(input_path,skiprows = 1,delimiter=',',dtype = 'float32',usecols = tuple(range(0,7))))\n","                traj = self.normalize_data(traj,\"end_full\")\n","\n","                x_pred = self.inverse_model(traj.to(self.device))\n","                x_pred = x_pred.cpu()\n","                x_pred = self.denormalize_data(x_pred,\"motor\").numpy(force=True)\n","\n","                np.savetxt(output_path,x_pred,delimiter=\",\",header=\"dtm_13,dtm_24,de_13,de_24,dl\",comments='')\n","                print(\"Trajectory saved!\")\n","\n"]},{"cell_type":"markdown","source":["# Function for Trajectory Interpolation"],"metadata":{"id":"gdOywbqrwVbd"}},{"cell_type":"code","source":["!pip install pyquaternion\n","import csv\n","import numpy as np\n","from pyquaternion import Quaternion\n","from scipy.interpolate import interp1d\n","\n","def interpolate_trajectory(input_filename, output_filename, dist_param):\n","    with open(input_filename, 'r') as f:\n","        reader = csv.reader(f)\n","        # Skip the header\n","        next(reader)\n","        data = list(reader)\n","\n","    # Convert data to float\n","    data = np.array(data, dtype=float)\n","\n","    # Get positions and quaternions\n","    pos = data[:, :3]\n","    quats = [Quaternion(q) for q in data[:, 3:]]\n","\n","    # Compute distances between consecutive positions\n","    dists = np.sqrt(np.sum(np.diff(pos, axis=0)**2, axis=1))\n","    dists = np.insert(dists, 0, 0)  # first distance is 0\n","\n","    # Compute cumulative distances along the path\n","    cum_dists = np.cumsum(dists)\n","\n","    # Compute total number of interpolated points\n","    total_dist = cum_dists[-1]\n","    num_interp_points = int(np.ceil(total_dist / dist_param))\n","\n","    # Create array for interpolated distances\n","    interp_dists = np.linspace(0, total_dist, num_interp_points)\n","\n","    # Interpolate positions\n","    interp_pos = np.zeros((num_interp_points, 3))\n","    for i in range(3):\n","        interp_func = interp1d(cum_dists, pos[:, i])\n","        interp_pos[:, i] = interp_func(interp_dists)\n","\n","    # Get the original index for each interpolated point\n","    interp_indices = np.searchsorted(cum_dists, interp_dists) - 1\n","\n","    # Interpolate quaternions using pyquaternion's Quaternion class\n","    interp_quats = np.zeros((num_interp_points, 4))\n","    epsilon = 1e-10  # Small constant to avoid division by zero\n","    for i in range(num_interp_points - 1):\n","        fraction = (interp_dists[i] - cum_dists[interp_indices[i]]) / (cum_dists[interp_indices[i+1]] - cum_dists[interp_indices[i]] + epsilon)\n","        interp_quats[i] = Quaternion.slerp(quats[interp_indices[i]], quats[interp_indices[i+1]], fraction).elements\n","\n","    # Assign the last original quaternion to the last interpolated point\n","    interp_quats[-1] = quats[-1].elements\n","\n","    interp_dists += 1\n","\n","    # Prepare data for writing to csv\n","    output_data = np.hstack((interp_pos, interp_quats, interp_indices.reshape(-1, 1)))\n","\n","    with open(output_filename, 'w', newline='') as f:\n","        writer = csv.writer(f)\n","        writer.writerow([\"x_end\", \"y_end\", \"z_end\", \"qx_end\", \"qy_end\", \"qz_end\", \"qw_end\", \"orig_index\"])\n","        writer.writerows(output_data)"],"metadata":{"id":"FmoWPzjMwUaq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695050999491,"user_tz":240,"elapsed":12004,"user":{"displayName":"Kuba Kowalewski","userId":"08857268640727880536"}},"outputId":"ff2ef30c-a948-49a6-9826-0718e8a0fc67"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyquaternion\n","  Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyquaternion) (1.23.5)\n","Installing collected packages: pyquaternion\n","Successfully installed pyquaternion-0.9.9\n"]}]},{"cell_type":"markdown","metadata":{"id":"qmrjj3naSC8_"},"source":["# Launch aux-bot"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"67SEhBfDSCgq","colab":{"base_uri":"https://localhost:8080/","height":442},"executionInfo":{"status":"error","timestamp":1695051018886,"user_tz":240,"elapsed":18002,"user":{"displayName":"Kuba Kowalewski","userId":"08857268640727880536"}},"outputId":"b693f6cb-13ab-4e04-f1a5-6da8e0f76c78"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n","Connected:  cuda:0\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-a29d147ae762>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Create bot and train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mbot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maux_bot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrive_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/data/15000_pose_data_07_26_2023.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_forward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Use inverse model for regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-2c1ce4ec092d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, drive_path, data_path, train_forward, train_inverse, normalize)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Upload data as fixed length trajectories instead of single points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSEQ_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Train forward model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-2c1ce4ec092d>\u001b[0m in \u001b[0;36mupload_data\u001b[0;34m(self, data_path, trim_length, batch_size, seq_length)\u001b[0m\n\u001b[1;32m    187\u001b[0m         '''\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m# Upload and convert units from m to mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         self.data = np.loadtxt(data_path, skiprows = 1, delimiter=',', dtype = 'float32',\n\u001b[0m\u001b[1;32m    190\u001b[0m                                usecols = tuple(range(2, 2 + self.input_size + self.output_size)))\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_slice_xyz\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m10e2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1336\u001b[0m         \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m     arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n\u001b[0m\u001b[1;32m   1339\u001b[0m                 \u001b[0mconverters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiplines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m                 \u001b[0munpack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    531\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path} not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: /gdrive/My Drive/Github/aux-net/data/15000_pose_data_07_26_2023.csv not found."]}],"source":["from google.colab import drive\n","\n","drive.mount('/gdrive')\n","drive_path = '/gdrive/My Drive/Github/aux-net'\n","\n","# Generate trajectory from waypoints\n","interpolate_trajectory(drive_path + '/experiments/waypoints_E1.csv',drive_path+'/experiments/trajectory_E1.csv',0.002)\n","\n","# Create bot and train\n","bot = aux_bot(drive_path = drive_path, data_path = \"/data/15000_pose_data_07_26_2023.csv\", train_forward=False, train_inverse=False, normalize=True)\n","\n","# Use inverse model for regression\n","# bot.inverse_prediction(input_path = drive_path+'/experiments/trajectory_E1.csv', output_path = drive_path+'/experiments/motor_trajectory_E1.csv')"]},{"cell_type":"markdown","source":["# Baysiean Optimization"],"metadata":{"id":"80GniJwG8ECq"}},{"cell_type":"code","source":["!pip install bayesian-optimization\n","from bayes_opt import BayesianOptimization\n","from google.colab import drive\n","import pickle\n","\n","drive.mount('/gdrive')\n","drive_path = '/gdrive/My Drive/Github/aux-net'\n","\n","# Create instance for optimization\n","bot = aux_bot(drive_path = drive_path, data_path = \"/data/15000_pose_data_07_26_2023.csv\", train_forward=False, train_inverse=False, normalize=True)\n","\n","# Define architecture and hyper parameter bounds\n","pbounds = {\n","    'lr': (1e-5, 1e-2),\n","    'hidden_lstm': (32,1024),\n","    'hidden': (32,2048),\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hggNqXRi8Dqv","executionInfo":{"status":"ok","timestamp":1692836750285,"user_tz":240,"elapsed":40272,"user":{"displayName":"Kuba Kowalewski","userId":"08857268640727880536"}},"outputId":"25b0670f-6d0a-4f5e-f916-a6751ce1dad2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bayesian-optimization\n","  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n","Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.23.5)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.10.1)\n","Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n","Collecting colorama>=0.4.6 (from bayesian-optimization)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.2.0)\n","Installing collected packages: colorama, bayesian-optimization\n","Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n","Mounted at /gdrive\n","Connected:  cuda:0\n","Inputs:  [[ 0.         0.         0.         0.         0.       ]\n"," [ 0.8101009 -1.0041184  1.1364819 -5.0179515  1.4013056]\n"," [-0.6131911 -1.9974661  2.8804388 -6.3800273  1.8499048]]\n","Outputs:  [[ 6.9101654e+01  3.2354901e+02  7.4590775e+01 -1.9104969e-02\n","  -9.8767841e-01  3.7367977e-02  1.5076485e-01]\n"," [ 7.4509354e+01  3.2411902e+02  7.6167366e+01 -3.4702506e-03\n","  -9.8758996e-01  4.5616228e-02  1.5024395e-01]\n"," [ 7.4593063e+01  3.2474231e+02  8.0255966e+01 -4.0545864e-03\n","  -9.8698211e-01  5.8374155e-02  1.4980762e-01]]\n","FORWARD sequences:\n","(tensor([[0.5544, 0.2315, 0.5764, 0.4341, 0.4955],\n","        [0.5262, 0.2426, 0.5493, 0.3776, 0.4737],\n","        [0.5161, 0.2295, 0.4814, 0.3661, 0.4656],\n","        [0.4604, 0.2582, 0.4149, 0.3470, 0.5028],\n","        [0.4596, 0.2848, 0.4111, 0.3513, 0.4613],\n","        [0.4866, 0.3043, 0.3974, 0.3754, 0.4021],\n","        [0.4566, 0.3384, 0.4116, 0.3539, 0.4055],\n","        [0.4685, 0.3513, 0.4751, 0.3306, 0.3810],\n","        [0.5097, 0.3655, 0.4643, 0.3454, 0.3452],\n","        [0.5129, 0.3359, 0.4113, 0.2991, 0.4029],\n","        [0.4721, 0.3533, 0.3783, 0.2948, 0.4116],\n","        [0.4306, 0.3618, 0.3735, 0.2881, 0.4698],\n","        [0.4064, 0.3728, 0.3706, 0.2783, 0.4475],\n","        [0.3719, 0.4212, 0.3633, 0.2403, 0.4427],\n","        [0.3838, 0.3587, 0.3196, 0.2016, 0.4936],\n","        [0.3815, 0.3564, 0.3141, 0.2433, 0.4476],\n","        [0.3984, 0.3617, 0.2352, 0.3584, 0.4122],\n","        [0.3904, 0.3889, 0.2325, 0.3598, 0.4061],\n","        [0.3773, 0.4172, 0.2369, 0.3546, 0.3597],\n","        [0.4335, 0.3948, 0.2906, 0.3420, 0.3172],\n","        [0.4410, 0.3867, 0.3418, 0.3414, 0.3258],\n","        [0.4804, 0.3885, 0.2953, 0.3344, 0.3333],\n","        [0.5099, 0.4020, 0.3817, 0.3191, 0.3244],\n","        [0.5275, 0.4472, 0.4449, 0.3022, 0.2843],\n","        [0.5190, 0.4309, 0.5420, 0.3166, 0.3190],\n","        [0.4886, 0.4285, 0.5973, 0.3244, 0.3666],\n","        [0.4737, 0.4131, 0.6060, 0.3378, 0.3889],\n","        [0.4956, 0.4414, 0.6145, 0.3962, 0.3194],\n","        [0.5212, 0.4286, 0.6311, 0.3647, 0.3695],\n","        [0.5431, 0.4631, 0.6254, 0.3818, 0.3276],\n","        [0.5423, 0.4741, 0.6667, 0.4068, 0.3369],\n","        [0.5865, 0.5087, 0.7446, 0.4671, 0.3504]]), tensor([0.5034, 0.4235, 0.5343, 0.4344, 0.2676, 0.6966, 0.4991]))\n","(tensor([[0.5205, 0.4839, 0.3088, 0.3579, 0.4128],\n","        [0.5404, 0.5179, 0.3291, 0.3262, 0.4063],\n","        [0.5440, 0.5518, 0.2736, 0.2893, 0.4644],\n","        [0.5022, 0.5414, 0.2941, 0.3384, 0.3943],\n","        [0.4835, 0.5514, 0.2428, 0.3707, 0.4250],\n","        [0.4913, 0.5418, 0.2524, 0.3889, 0.4077],\n","        [0.4602, 0.5199, 0.2571, 0.3713, 0.4171],\n","        [0.4215, 0.5188, 0.2768, 0.3226, 0.4363],\n","        [0.4013, 0.5526, 0.2463, 0.3252, 0.4945],\n","        [0.4019, 0.5518, 0.2715, 0.3318, 0.5140],\n","        [0.4241, 0.5704, 0.3101, 0.3493, 0.5046],\n","        [0.4266, 0.5721, 0.3551, 0.3697, 0.4738],\n","        [0.4133, 0.5851, 0.3626, 0.3534, 0.5192],\n","        [0.4395, 0.5970, 0.3366, 0.3528, 0.5377],\n","        [0.4074, 0.5655, 0.3457, 0.3702, 0.5420],\n","        [0.4055, 0.5683, 0.3609, 0.3346, 0.5743],\n","        [0.4377, 0.5852, 0.3931, 0.2922, 0.5792],\n","        [0.4763, 0.6085, 0.3611, 0.3172, 0.5768],\n","        [0.4673, 0.6318, 0.3530, 0.2929, 0.6028],\n","        [0.4405, 0.6543, 0.3245, 0.3081, 0.6436],\n","        [0.4281, 0.6034, 0.3924, 0.3307, 0.5988],\n","        [0.4101, 0.5896, 0.3588, 0.3761, 0.6058],\n","        [0.3778, 0.5905, 0.3017, 0.3530, 0.6515],\n","        [0.3931, 0.5582, 0.2682, 0.3973, 0.6149],\n","        [0.4152, 0.5802, 0.2810, 0.4342, 0.5794],\n","        [0.4182, 0.6033, 0.2330, 0.4532, 0.6280],\n","        [0.4507, 0.6024, 0.3410, 0.4303, 0.5672],\n","        [0.4423, 0.6112, 0.3489, 0.3886, 0.5602],\n","        [0.4357, 0.6172, 0.3364, 0.3876, 0.5496],\n","        [0.4297, 0.5829, 0.3714, 0.4210, 0.5236],\n","        [0.4047, 0.5802, 0.3963, 0.3471, 0.5478],\n","        [0.3966, 0.5532, 0.4837, 0.3770, 0.5114]]), tensor([0.6092, 0.2754, 0.3625, 0.5996, 0.1432, 0.5692, 0.4762]))\n","INVERSE sequences:\n","(tensor([[0.7947, 0.3771, 0.5750, 0.4943, 0.1124, 0.6000, 0.4059],\n","        [0.8090, 0.3880, 0.5632, 0.5243, 0.1151, 0.6023, 0.4110],\n","        [0.8117, 0.3858, 0.5510, 0.5304, 0.1110, 0.5934, 0.4149],\n","        [0.8514, 0.3841, 0.4601, 0.5764, 0.0975, 0.5315, 0.4399],\n","        [0.8410, 0.3897, 0.4390, 0.6028, 0.1071, 0.5223, 0.4469],\n","        [0.8117, 0.3930, 0.4179, 0.6176, 0.1137, 0.4948, 0.4624],\n","        [0.7855, 0.3824, 0.4031, 0.6231, 0.1188, 0.4974, 0.4653],\n","        [0.7560, 0.3924, 0.4290, 0.6126, 0.1282, 0.5625, 0.4429],\n","        [0.7264, 0.3839, 0.4359, 0.6033, 0.1319, 0.5646, 0.4566],\n","        [0.7606, 0.4098, 0.4410, 0.6563, 0.1697, 0.5638, 0.4711],\n","        [0.7756, 0.4242, 0.4243, 0.6828, 0.1855, 0.5527, 0.4729],\n","        [0.7821, 0.4158, 0.3910, 0.6972, 0.1844, 0.5461, 0.4523],\n","        [0.7927, 0.4294, 0.3644, 0.7096, 0.1931, 0.5522, 0.4400],\n","        [0.7687, 0.4296, 0.3474, 0.7201, 0.2126, 0.5626, 0.4464],\n","        [0.8145, 0.4712, 0.3545, 0.7681, 0.2713, 0.5732, 0.4387],\n","        [0.8236, 0.4795, 0.3602, 0.7749, 0.2815, 0.5756, 0.4380],\n","        [0.7855, 0.4596, 0.3171, 0.7314, 0.2074, 0.4869, 0.4687],\n","        [0.7625, 0.4698, 0.2920, 0.7213, 0.1952, 0.4472, 0.4751],\n","        [0.7538, 0.4696, 0.2862, 0.7179, 0.1886, 0.4433, 0.4695],\n","        [0.7386, 0.4649, 0.3083, 0.7038, 0.1802, 0.4550, 0.4794],\n","        [0.7304, 0.4577, 0.3376, 0.6973, 0.1702, 0.4665, 0.4701],\n","        [0.7344, 0.4555, 0.3576, 0.7073, 0.1874, 0.4707, 0.4836],\n","        [0.7041, 0.4438, 0.4130, 0.6854, 0.1749, 0.5068, 0.4833],\n","        [0.6672, 0.4308, 0.4315, 0.6653, 0.1753, 0.5436, 0.4884],\n","        [0.6360, 0.4387, 0.4966, 0.6167, 0.2077, 0.6345, 0.4926],\n","        [0.6243, 0.4369, 0.5167, 0.5879, 0.2434, 0.6807, 0.4978],\n","        [0.6025, 0.4235, 0.5136, 0.5621, 0.2302, 0.6760, 0.4967],\n","        [0.5819, 0.4223, 0.5060, 0.5319, 0.2199, 0.6686, 0.5005],\n","        [0.5826, 0.4217, 0.5097, 0.5327, 0.2220, 0.6693, 0.5028],\n","        [0.5635, 0.4226, 0.5115, 0.5205, 0.2108, 0.6613, 0.4985],\n","        [0.5516, 0.4309, 0.5250, 0.5066, 0.2360, 0.6805, 0.5069],\n","        [0.5034, 0.4235, 0.5343, 0.4344, 0.2676, 0.6966, 0.4991]]), tensor([0.5865, 0.5087, 0.7446, 0.4671, 0.3504]))\n","(tensor([[0.6552, 0.4106, 0.3545, 0.7200, 0.2159, 0.4754, 0.5103],\n","        [0.6582, 0.4177, 0.3717, 0.7283, 0.2149, 0.4848, 0.4896],\n","        [0.6677, 0.4252, 0.3735, 0.7502, 0.2448, 0.4964, 0.4932],\n","        [0.6632, 0.4387, 0.3490, 0.7524, 0.2402, 0.4801, 0.4866],\n","        [0.6501, 0.4363, 0.3133, 0.7419, 0.2221, 0.4423, 0.4829],\n","        [0.6337, 0.4286, 0.3073, 0.7254, 0.2065, 0.4261, 0.4871],\n","        [0.6412, 0.4277, 0.3033, 0.7250, 0.2086, 0.4249, 0.4915],\n","        [0.6908, 0.4384, 0.3187, 0.7570, 0.2506, 0.4566, 0.5002],\n","        [0.6823, 0.4356, 0.2969, 0.7844, 0.2764, 0.4496, 0.4826],\n","        [0.6569, 0.3965, 0.3119, 0.7578, 0.2382, 0.4703, 0.4733],\n","        [0.6286, 0.3680, 0.3150, 0.7248, 0.2025, 0.4813, 0.4749],\n","        [0.5962, 0.3468, 0.3208, 0.6846, 0.1645, 0.4943, 0.4716],\n","        [0.5981, 0.3316, 0.3224, 0.6830, 0.1637, 0.5075, 0.4659],\n","        [0.5822, 0.3288, 0.3283, 0.6885, 0.1670, 0.5068, 0.4634],\n","        [0.5986, 0.3129, 0.3170, 0.6788, 0.1553, 0.4894, 0.4655],\n","        [0.6161, 0.3064, 0.3240, 0.6886, 0.1699, 0.5049, 0.4699],\n","        [0.6106, 0.3081, 0.3608, 0.7150, 0.2093, 0.5458, 0.4673],\n","        [0.5969, 0.3071, 0.3727, 0.7187, 0.2174, 0.5471, 0.4740],\n","        [0.5839, 0.3143, 0.3673, 0.7244, 0.2219, 0.5500, 0.4683],\n","        [0.5586, 0.3188, 0.3229, 0.7228, 0.1983, 0.5256, 0.4469],\n","        [0.5767, 0.2870, 0.3449, 0.6874, 0.1732, 0.5370, 0.4557],\n","        [0.5826, 0.2719, 0.3212, 0.6641, 0.1436, 0.5094, 0.4553],\n","        [0.6090, 0.2829, 0.2783, 0.6920, 0.1577, 0.4845, 0.4501],\n","        [0.6301, 0.2943, 0.2704, 0.6978, 0.1723, 0.4343, 0.4759],\n","        [0.6019, 0.2977, 0.2704, 0.6709, 0.1461, 0.4201, 0.4686],\n","        [0.5827, 0.2927, 0.2529, 0.6601, 0.1397, 0.3747, 0.4549],\n","        [0.5479, 0.2722, 0.3097, 0.6111, 0.1030, 0.4371, 0.4607],\n","        [0.5543, 0.2848, 0.3290, 0.6441, 0.1250, 0.4830, 0.4593],\n","        [0.5664, 0.2926, 0.3220, 0.6552, 0.1302, 0.4767, 0.4569],\n","        [0.5710, 0.2858, 0.3279, 0.6373, 0.1240, 0.4802, 0.4667],\n","        [0.5971, 0.2955, 0.3370, 0.6544, 0.1459, 0.5238, 0.4639],\n","        [0.6092, 0.2754, 0.3625, 0.5996, 0.1432, 0.5692, 0.4762]]), tensor([0.3966, 0.5532, 0.4837, 0.3770, 0.5114]))\n","[aux_net] Forward model succesfully loaded\n","[aux_net] Inverse model succesfully loaded\n"]}]},{"cell_type":"code","source":["# Define optimizer\n","optimizer = BayesianOptimization(\n","    f=bot.train_validate,\n","    pbounds=pbounds,\n","    random_state=1,\n",")\n","\n","# Run optimization\n","optimizer.maximize(\n","    init_points=25,\n","    n_iter=40\n",")\n","\n","# Print best parameters\n","print(\"Best parameters:\", optimizer.max['params'])\n","\n","# Save results\n","with open('bayesian_opt_results.pkl', 'wb') as f:\n","    pickle.dump(optimizer.res, f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0RKZZOU-TUai","executionInfo":{"status":"ok","timestamp":1692849127452,"user_tz":240,"elapsed":12377171,"user":{"displayName":"Kuba Kowalewski","userId":"08857268640727880536"}},"outputId":"db200639-73db-4da3-9cee-7b6799621638"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["|   iter    |  target   |  hidden   | hidden... |    lr     |\n","-------------------------------------------------------------\n","| \u001b[0m1        \u001b[0m | \u001b[0m-15.59   \u001b[0m | \u001b[0m872.7    \u001b[0m | \u001b[0m746.6    \u001b[0m | \u001b[0m1.114e-05\u001b[0m |\n","| \u001b[95m2        \u001b[0m | \u001b[95m-5.235   \u001b[0m | \u001b[95m641.5    \u001b[0m | \u001b[95m177.6    \u001b[0m | \u001b[95m0.0009325\u001b[0m |\n","| \u001b[95m3        \u001b[0m | \u001b[95m-4.683   \u001b[0m | \u001b[95m407.5    \u001b[0m | \u001b[95m374.8    \u001b[0m | \u001b[95m0.003974 \u001b[0m |\n","| \u001b[0m4        \u001b[0m | \u001b[0m-4.846   \u001b[0m | \u001b[0m1.118e+03\u001b[0m | \u001b[0m447.8    \u001b[0m | \u001b[0m0.006855 \u001b[0m |\n","| \u001b[0m5        \u001b[0m | \u001b[0m-5.101   \u001b[0m | \u001b[0m444.2    \u001b[0m | \u001b[0m903.1    \u001b[0m | \u001b[0m0.0002836\u001b[0m |\n","| \u001b[0m6        \u001b[0m | \u001b[0m-4.702   \u001b[0m | \u001b[0m1.384e+03\u001b[0m | \u001b[0m446.0    \u001b[0m | \u001b[0m0.005591 \u001b[0m |\n","| \u001b[0m7        \u001b[0m | \u001b[0m-5.12    \u001b[0m | \u001b[0m315.0    \u001b[0m | \u001b[0m228.5    \u001b[0m | \u001b[0m0.008009 \u001b[0m |\n","| \u001b[0m8        \u001b[0m | \u001b[0m-4.905   \u001b[0m | \u001b[0m1.984e+03\u001b[0m | \u001b[0m342.9    \u001b[0m | \u001b[0m0.006926 \u001b[0m |\n","| \u001b[95m9        \u001b[0m | \u001b[95m-4.43    \u001b[0m | \u001b[95m1.799e+03\u001b[0m | \u001b[95m919.4    \u001b[0m | \u001b[95m0.0008596\u001b[0m |\n","| \u001b[0m10       \u001b[0m | \u001b[0m-5.269   \u001b[0m | \u001b[0m110.7    \u001b[0m | \u001b[0m200.5    \u001b[0m | \u001b[0m0.008783 \u001b[0m |\n","| \u001b[0m11       \u001b[0m | \u001b[0m-26.79   \u001b[0m | \u001b[0m230.3    \u001b[0m | \u001b[0m449.7    \u001b[0m | \u001b[0m0.009579 \u001b[0m |\n","| \u001b[0m12       \u001b[0m | \u001b[0m-4.569   \u001b[0m | \u001b[0m1.107e+03\u001b[0m | \u001b[0m718.3    \u001b[0m | \u001b[0m0.003162 \u001b[0m |\n","| \u001b[0m13       \u001b[0m | \u001b[0m-4.996   \u001b[0m | \u001b[0m1.416e+03\u001b[0m | \u001b[0m859.9    \u001b[0m | \u001b[0m0.0001927\u001b[0m |\n","| \u001b[0m14       \u001b[0m | \u001b[0m-26.79   \u001b[0m | \u001b[0m1.544e+03\u001b[0m | \u001b[0m1.013e+03\u001b[0m | \u001b[0m0.007484 \u001b[0m |\n","| \u001b[0m15       \u001b[0m | \u001b[0m-4.641   \u001b[0m | \u001b[0m597.4    \u001b[0m | \u001b[0m815.0    \u001b[0m | \u001b[0m0.001041 \u001b[0m |\n","| \u001b[0m16       \u001b[0m | \u001b[0m-5.125   \u001b[0m | \u001b[0m935.0    \u001b[0m | \u001b[0m933.3    \u001b[0m | \u001b[0m0.002943 \u001b[0m |\n","| \u001b[0m17       \u001b[0m | \u001b[0m-6.566   \u001b[0m | \u001b[0m612.2    \u001b[0m | \u001b[0m161.0    \u001b[0m | \u001b[0m0.0002035\u001b[0m |\n","| \u001b[0m18       \u001b[0m | \u001b[0m-4.746   \u001b[0m | \u001b[0m1.401e+03\u001b[0m | \u001b[0m241.9    \u001b[0m | \u001b[0m0.002663 \u001b[0m |\n","| \u001b[0m19       \u001b[0m | \u001b[0m-5.449   \u001b[0m | \u001b[0m1.023e+03\u001b[0m | \u001b[0m84.94    \u001b[0m | \u001b[0m0.005745 \u001b[0m |\n","| \u001b[0m20       \u001b[0m | \u001b[0m-26.79   \u001b[0m | \u001b[0m327.8    \u001b[0m | \u001b[0m616.6    \u001b[0m | \u001b[0m0.007001 \u001b[0m |\n","| \u001b[0m21       \u001b[0m | \u001b[0m-5.362   \u001b[0m | \u001b[0m238.3    \u001b[0m | \u001b[0m442.7    \u001b[0m | \u001b[0m0.006947 \u001b[0m |\n","| \u001b[0m22       \u001b[0m | \u001b[0m-5.467   \u001b[0m | \u001b[0m867.0    \u001b[0m | \u001b[0m81.55    \u001b[0m | \u001b[0m0.005364 \u001b[0m |\n","| \u001b[0m23       \u001b[0m | \u001b[0m-26.79   \u001b[0m | \u001b[0m1.37e+03 \u001b[0m | \u001b[0m542.8    \u001b[0m | \u001b[0m0.009447 \u001b[0m |\n","| \u001b[0m24       \u001b[0m | \u001b[0m-4.627   \u001b[0m | \u001b[0m1.214e+03\u001b[0m | \u001b[0m928.2    \u001b[0m | \u001b[0m0.001383 \u001b[0m |\n","| \u001b[0m25       \u001b[0m | \u001b[0m-5.287   \u001b[0m | \u001b[0m312.8    \u001b[0m | \u001b[0m832.9    \u001b[0m | \u001b[0m0.003983 \u001b[0m |\n","| \u001b[0m26       \u001b[0m | \u001b[0m-5.035   \u001b[0m | \u001b[0m610.8    \u001b[0m | \u001b[0m160.1    \u001b[0m | \u001b[0m0.004639 \u001b[0m |\n","| \u001b[0m27       \u001b[0m | \u001b[0m-26.79   \u001b[0m | \u001b[0m1.206e+03\u001b[0m | \u001b[0m924.2    \u001b[0m | \u001b[0m0.006775 \u001b[0m |\n","| \u001b[0m28       \u001b[0m | \u001b[0m-14.23   \u001b[0m | \u001b[0m1.22e+03 \u001b[0m | \u001b[0m930.8    \u001b[0m | \u001b[0m1e-05    \u001b[0m |\n","| \u001b[0m29       \u001b[0m | \u001b[0m-4.781   \u001b[0m | \u001b[0m1.102e+03\u001b[0m | \u001b[0m713.7    \u001b[0m | \u001b[0m0.005475 \u001b[0m |\n","| \u001b[0m30       \u001b[0m | \u001b[0m-5.623   \u001b[0m | \u001b[0m1.099e+03\u001b[0m | \u001b[0m719.8    \u001b[0m | \u001b[0m0.009221 \u001b[0m |\n","| \u001b[0m31       \u001b[0m | \u001b[0m-26.79   \u001b[0m | \u001b[0m1.103e+03\u001b[0m | \u001b[0m725.5    \u001b[0m | \u001b[0m0.006567 \u001b[0m |\n","| \u001b[0m32       \u001b[0m | \u001b[0m-4.766   \u001b[0m | \u001b[0m412.5    \u001b[0m | \u001b[0m375.7    \u001b[0m | \u001b[0m0.00406  \u001b[0m |\n","| \u001b[0m33       \u001b[0m | \u001b[0m-4.802   \u001b[0m | \u001b[0m412.3    \u001b[0m | \u001b[0m368.4    \u001b[0m | \u001b[0m0.002189 \u001b[0m |\n","| \u001b[0m34       \u001b[0m | \u001b[0m-4.769   \u001b[0m | \u001b[0m1.406e+03\u001b[0m | \u001b[0m239.9    \u001b[0m | \u001b[0m0.003219 \u001b[0m |\n","| \u001b[0m35       \u001b[0m | \u001b[0m-4.85    \u001b[0m | \u001b[0m1.402e+03\u001b[0m | \u001b[0m234.8    \u001b[0m | \u001b[0m0.004471 \u001b[0m |\n","| \u001b[0m36       \u001b[0m | \u001b[0m-4.615   \u001b[0m | \u001b[0m1.108e+03\u001b[0m | \u001b[0m710.5    \u001b[0m | \u001b[0m0.00284  \u001b[0m |\n","| \u001b[0m37       \u001b[0m | \u001b[0m-4.874   \u001b[0m | \u001b[0m1.377e+03\u001b[0m | \u001b[0m446.4    \u001b[0m | \u001b[0m0.005569 \u001b[0m |\n","| \u001b[0m38       \u001b[0m | \u001b[0m-5.002   \u001b[0m | \u001b[0m1.394e+03\u001b[0m | \u001b[0m233.1    \u001b[0m | \u001b[0m0.008482 \u001b[0m |\n","| \u001b[0m39       \u001b[0m | \u001b[0m-4.475   \u001b[0m | \u001b[0m1.807e+03\u001b[0m | \u001b[0m921.2    \u001b[0m | \u001b[0m0.001816 \u001b[0m |\n","| \u001b[0m40       \u001b[0m | \u001b[0m-4.607   \u001b[0m | \u001b[0m1.8e+03  \u001b[0m | \u001b[0m925.3    \u001b[0m | \u001b[0m0.003556 \u001b[0m |\n","| \u001b[0m41       \u001b[0m | \u001b[0m-4.922   \u001b[0m | \u001b[0m244.3    \u001b[0m | \u001b[0m436.5    \u001b[0m | \u001b[0m0.00582  \u001b[0m |\n","| \u001b[0m42       \u001b[0m | \u001b[0m-8.809   \u001b[0m | \u001b[0m1.383e+03\u001b[0m | \u001b[0m438.7    \u001b[0m | \u001b[0m0.007121 \u001b[0m |\n","| \u001b[0m43       \u001b[0m | \u001b[0m-4.662   \u001b[0m | \u001b[0m1.381e+03\u001b[0m | \u001b[0m456.3    \u001b[0m | \u001b[0m0.003143 \u001b[0m |\n","| \u001b[0m44       \u001b[0m | \u001b[0m-5.018   \u001b[0m | \u001b[0m602.7    \u001b[0m | \u001b[0m160.6    \u001b[0m | \u001b[0m0.002723 \u001b[0m |\n","| \u001b[0m45       \u001b[0m | \u001b[0m-5.014   \u001b[0m | \u001b[0m1.088e+03\u001b[0m | \u001b[0m716.3    \u001b[0m | \u001b[0m0.00393  \u001b[0m |\n","| \u001b[0m46       \u001b[0m | \u001b[0m-4.764   \u001b[0m | \u001b[0m1.115e+03\u001b[0m | \u001b[0m712.8    \u001b[0m | \u001b[0m0.0004473\u001b[0m |\n","| \u001b[0m47       \u001b[0m | \u001b[0m-5.109   \u001b[0m | \u001b[0m1.388e+03\u001b[0m | \u001b[0m242.8    \u001b[0m | \u001b[0m0.00765  \u001b[0m |\n","| \u001b[0m48       \u001b[0m | \u001b[0m-20.99   \u001b[0m | \u001b[0m248.2    \u001b[0m | \u001b[0m445.7    \u001b[0m | \u001b[0m0.009107 \u001b[0m |\n","| \u001b[0m49       \u001b[0m | \u001b[0m-26.79   \u001b[0m | \u001b[0m1.374e+03\u001b[0m | \u001b[0m454.0    \u001b[0m | \u001b[0m0.009754 \u001b[0m |\n","| \u001b[0m50       \u001b[0m | \u001b[0m-4.731   \u001b[0m | \u001b[0m1.095e+03\u001b[0m | \u001b[0m714.8    \u001b[0m | \u001b[0m0.005676 \u001b[0m |\n","| \u001b[0m51       \u001b[0m | \u001b[0m-4.986   \u001b[0m | \u001b[0m1.388e+03\u001b[0m | \u001b[0m452.5    \u001b[0m | \u001b[0m0.0003508\u001b[0m |\n","| \u001b[0m52       \u001b[0m | \u001b[0m-26.79   \u001b[0m | \u001b[0m596.4    \u001b[0m | \u001b[0m807.6    \u001b[0m | \u001b[0m0.007946 \u001b[0m |\n","| \u001b[0m53       \u001b[0m | \u001b[0m-5.132   \u001b[0m | \u001b[0m604.7    \u001b[0m | \u001b[0m155.0    \u001b[0m | \u001b[0m0.001354 \u001b[0m |\n","| \u001b[0m54       \u001b[0m | \u001b[0m-26.79   \u001b[0m | \u001b[0m603.5    \u001b[0m | \u001b[0m818.8    \u001b[0m | \u001b[0m0.009626 \u001b[0m |\n","| \u001b[0m55       \u001b[0m | \u001b[0m-26.79   \u001b[0m | \u001b[0m1.39e+03 \u001b[0m | \u001b[0m446.9    \u001b[0m | \u001b[0m0.008947 \u001b[0m |\n","| \u001b[0m56       \u001b[0m | \u001b[0m-4.761   \u001b[0m | \u001b[0m1.099e+03\u001b[0m | \u001b[0m715.0    \u001b[0m | \u001b[0m0.004237 \u001b[0m |\n","| \u001b[0m57       \u001b[0m | \u001b[0m-4.872   \u001b[0m | \u001b[0m1.387e+03\u001b[0m | \u001b[0m237.5    \u001b[0m | \u001b[0m0.005564 \u001b[0m |\n","| \u001b[0m58       \u001b[0m | \u001b[0m-26.79   \u001b[0m | \u001b[0m1.805e+03\u001b[0m | \u001b[0m913.4    \u001b[0m | \u001b[0m0.008052 \u001b[0m |\n","| \u001b[0m59       \u001b[0m | \u001b[0m-26.79   \u001b[0m | \u001b[0m1.805e+03\u001b[0m | \u001b[0m928.6    \u001b[0m | \u001b[0m0.003603 \u001b[0m |\n","| \u001b[0m60       \u001b[0m | \u001b[0m-6.461   \u001b[0m | \u001b[0m1.794e+03\u001b[0m | \u001b[0m921.2    \u001b[0m | \u001b[0m0.003759 \u001b[0m |\n","| \u001b[0m61       \u001b[0m | \u001b[0m-4.795   \u001b[0m | \u001b[0m1.406e+03\u001b[0m | \u001b[0m232.1    \u001b[0m | \u001b[0m0.002794 \u001b[0m |\n","| \u001b[0m62       \u001b[0m | \u001b[0m-4.57    \u001b[0m | \u001b[0m1.386e+03\u001b[0m | \u001b[0m456.1    \u001b[0m | \u001b[0m0.002195 \u001b[0m |\n","| \u001b[0m63       \u001b[0m | \u001b[0m-4.799   \u001b[0m | \u001b[0m1.401e+03\u001b[0m | \u001b[0m247.8    \u001b[0m | \u001b[0m0.003582 \u001b[0m |\n","| \u001b[0m64       \u001b[0m | \u001b[0m-4.862   \u001b[0m | \u001b[0m1.394e+03\u001b[0m | \u001b[0m248.6    \u001b[0m | \u001b[0m0.005504 \u001b[0m |\n","| \u001b[0m65       \u001b[0m | \u001b[0m-4.848   \u001b[0m | \u001b[0m418.6    \u001b[0m | \u001b[0m368.5    \u001b[0m | \u001b[0m0.001933 \u001b[0m |\n","=============================================================\n","Best parameters: {'hidden': 1798.8005310288133, 'hidden_lstm': 919.4498101958166, 'lr': 0.0008595916715840814}\n"]}]},{"cell_type":"markdown","source":["# GitHub"],"metadata":{"id":"kTnnw0hYArQt"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive')\n","\n","%cd '/gdrive/My Drive/Github/aux-net'\n","!git config --global user.email \"kubekk101@gmail.com\"\n","!git config --global user.name \"KubaSrc\"\n"],"metadata":{"id":"jkgOcybGyvwp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695048765085,"user_tz":240,"elapsed":22224,"user":{"displayName":"Kuba Kowalewski","userId":"08857268640727880536"}},"outputId":"1b05e689-e7c0-44d8-dab6-b9df4c128add"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n","/gdrive/My Drive/Github/aux-net\n"]}]},{"cell_type":"code","source":["!git branch --set-upstream-to=origin/main main\n","!git pull"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XSg5tXEnxwdQ","executionInfo":{"status":"ok","timestamp":1695049266975,"user_tz":240,"elapsed":102538,"user":{"displayName":"Kuba Kowalewski","userId":"08857268640727880536"}},"outputId":"37dc6982-957d-4737-8a4c-bf2cfb2011a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Branch 'main' set up to track remote branch 'main' from 'origin'.\n","Updating a91a8c5..946767c\n","Updating files: 100% (102/102), done.\n","Fast-forward\n"," .../15000_pose_data_07_23_2023_GAUSS.csv           | 30002 \u001b[32m+++++\u001b[m\u001b[31m-----\u001b[m\n"," .../15000_pose_data_07_23_2023_GAUSS_FORWARD.csv   |     0\n"," .../15000_pose_data_07_23_2023_GAUSS_INVERSE.csv   |     0\n"," data/{ => data-old}/15000_pose_data_07_25_2023.csv | 30002 \u001b[32m+++++\u001b[m\u001b[31m-----\u001b[m\n"," .../15000_pose_data_07_25_2023_FORWARD.csv         |     0\n"," .../15000_pose_data_07_25_2023_INVERSE.csv         |     0\n"," .../15000_pose_data_07_26_2023_FORWARD.csv         |     0\n"," .../15000_pose_data_07_26_2023_INVERSE.csv         |     0\n"," data/{ => data-old}/30000_pose_data_06_29_2023.csv | 60002 \u001b[32m+++++++++\u001b[m\u001b[31m----------\u001b[m\n"," data/{ => data-old}/30000_pose_data_07_05_2023.csv | 60002 \u001b[32m+++++++++\u001b[m\u001b[31m----------\u001b[m\n"," data/{ => data-old}/30000_pose_data_07_17_2023.csv | 60002 \u001b[32m+++++++++\u001b[m\u001b[31m----------\u001b[m\n"," data/{ => data-old}/30000_pose_data_07_20_2023.csv | 10002 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n"," data/{ => data-old}/30000_pose_data_07_21_2023.csv | 60002 \u001b[32m+++++++++\u001b[m\u001b[31m----------\u001b[m\n"," .../30000_pose_data_07_21_2023_FORWARD.csv         |     0\n"," .../30000_pose_data_07_21_2023_INVERSE.csv         |     0\n"," .../30000_pose_data_07_21_2023_MOTOR_POS.csv       | 60002 \u001b[32m+++++++++\u001b[m\u001b[31m----------\u001b[m\n"," .../30000_pose_raw_data_07_05_2023.csv             | 60002 \u001b[32m+++++++++\u001b[m\u001b[31m----------\u001b[m\n"," data/{ => data-old}/compensation_data.csv          |  2002 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n"," data/{ => data-old}/inverse_7500_train_data.csv    | 15002 \u001b[32m++\u001b[m\u001b[31m---\u001b[m\n"," data/{ => data-old}/inverse_compensation_data.csv  | 15002 \u001b[32m++\u001b[m\u001b[31m---\u001b[m\n"," .../inverse_compensation_motor_data.csv            | 15002 \u001b[32m++\u001b[m\u001b[31m---\u001b[m\n"," data/{ => full}/15000_pose_data_07_26_2023.csv     | 30002 \u001b[32m+++++\u001b[m\u001b[31m-----\u001b[m\n"," data/repetability/repeatability_07_26_2023.csv     |   501 \u001b[32m+\u001b[m\n"," figures/repetability/pos_error.png                 |   Bin \u001b[31m0\u001b[m -> \u001b[32m261684\u001b[m bytes\n"," figures/repetability/pos_hist.png                  |   Bin \u001b[31m0\u001b[m -> \u001b[32m105391\u001b[m bytes\n"," figures/repetability/quat_error.png                |   Bin \u001b[31m0\u001b[m -> \u001b[32m276747\u001b[m bytes\n"," figures/repetability/quat_hist.png                 |   Bin \u001b[31m0\u001b[m -> \u001b[32m96061\u001b[m bytes\n"," figures/workspace/z-x.png                          |   Bin \u001b[31m0\u001b[m -> \u001b[32m378640\u001b[m bytes\n"," matlab/DAQ Experiments/fifdeg.png                  |   Bin \u001b[31m0\u001b[m -> \u001b[32m57498\u001b[m bytes\n"," matlab/DAQ Experiments/fifdeg.txt                  |  8000 \u001b[32m+++\u001b[m\n"," matlab/DAQ Experiments/fivedeg.png                 |   Bin \u001b[31m0\u001b[m -> \u001b[32m60876\u001b[m bytes\n"," matlab/DAQ Experiments/fivedeg.txt                 | 10000 \u001b[32m+++\u001b[m\n"," matlab/DAQ Experiments/shear25mm.png               |   Bin \u001b[31m0\u001b[m -> \u001b[32m60549\u001b[m bytes\n"," matlab/DAQ Experiments/shear25mm.txt               | 10000 \u001b[32m+++\u001b[m\n"," matlab/DAQ Experiments/tendeg.png                  |   Bin \u001b[31m0\u001b[m -> \u001b[32m59009\u001b[m bytes\n"," matlab/DAQ Experiments/tendeg.txt                  |  8500 \u001b[32m+++\u001b[m\n"," matlab/DAQ Experiments/test.m                      |    94 \u001b[32m+\u001b[m\n"," matlab/DAQ Experiments/twendeg.png                 |   Bin \u001b[31m0\u001b[m -> \u001b[32m57398\u001b[m bytes\n"," matlab/DAQ Experiments/twendeg.txt                 |  7500 \u001b[32m+++\u001b[m\n"," matlab/DAQ Experiments/zerozero.png                |   Bin \u001b[31m0\u001b[m -> \u001b[32m56486\u001b[m bytes\n"," matlab/DAQ Experiments/zerozero.txt                | 10000 \u001b[32m+++\u001b[m\n"," matlab/Tensile Experiments/minus13mm.txt           |   130 \u001b[32m+\u001b[m\n"," matlab/Tensile Experiments/minus6p5mm.txt          |   180 \u001b[32m+\u001b[m\n"," matlab/Tensile Experiments/plus13mm.txt            |   130 \u001b[32m+\u001b[m\n"," matlab/Tensile Experiments/plus22p5mm.txt          |   130 \u001b[32m+\u001b[m\n"," matlab/Tensile Experiments/plus6p5mm.txt           |   150 \u001b[32m+\u001b[m\n"," matlab/Tensile Experiments/zeromm.txt              |   213 \u001b[32m+\u001b[m\n"," matlab/brewermap.m                                 |   511 \u001b[32m+\u001b[m\n"," matlab/dL-tracking.svg                             |   224 \u001b[32m+\u001b[m\n"," matlab/instron/convert-data.ipynb                  |   409 \u001b[32m+\u001b[m\n"," matlab/instron/flex-shaft.csv                      |    16 \u001b[32m+\u001b[m\n"," matlab/instron/flex-shaft.svg                      |     1 \u001b[32m+\u001b[m\n"," matlab/instron/old/simple-azimuthal.csv            |  3324 \u001b[32m+\u001b[m\n"," matlab/instron/old/simple-polar.csv                |  2224 \u001b[32m+\u001b[m\n"," matlab/instron/old/truss-azimuthal.csv             |  3324 \u001b[32m+\u001b[m\n"," matlab/instron/simple-printed-force.csv            |   243 \u001b[32m+\u001b[m\n"," matlab/instron/simple-printed-force.svg            |     1 \u001b[32m+\u001b[m\n"," matlab/instron/simple-printed-torque.svg           |     1 \u001b[32m+\u001b[m\n"," matlab/instron/simple-printed-torque_azimuth.csv   |   203 \u001b[32m+\u001b[m\n"," matlab/instron/simple-printed-torque_pole.csv      |   203 \u001b[32m+\u001b[m\n"," matlab/instron/simple-steel-force.csv              |   453 \u001b[32m+\u001b[m\n"," matlab/instron/simple-steel-force.svg              |     1 \u001b[32m+\u001b[m\n"," matlab/instron/simple-steel-torque.svg             |     1 \u001b[32m+\u001b[m\n"," matlab/instron/simple-steel-torque_azimuth.csv     |   205 \u001b[32m+\u001b[m\n"," matlab/instron/simple-steel-torque_pole.csv        |   203 \u001b[32m+\u001b[m\n"," matlab/instron/truss-steel-force.csv               |   453 \u001b[32m+\u001b[m\n"," matlab/instron/truss-steel-force.svg               |     1 \u001b[32m+\u001b[m\n"," matlab/instron/truss-steel-torque.svg              |     1 \u001b[32m+\u001b[m\n"," matlab/instron/truss-steel-torque_azimuth.csv      |   205 \u001b[32m+\u001b[m\n"," matlab/instron/truss-steel-torque_pole.csv         |   199 \u001b[32m+\u001b[m\n"," matlab/instron_plots.m                             |   217 \u001b[32m+\u001b[m\n"," matlab/plot_cvjoint.m                              |   125 \u001b[32m+\u001b[m\n"," matlab/repetability_analysis.m                     |   255 \u001b[32m+\u001b[m\n"," matlab/theta-tracking.svg                          |   252 \u001b[32m+\u001b[m\n"," ...8.550mm => forward_2023_09_13-17_16_55_8.550mm} |   Bin\n"," ...8.471mm => forward_2023_09_13-17_55_59_8.471mm} |   Bin\n"," ...6mm => forward_2023_09_13-23_01_10_7_496mm.mat} |   Bin\n"," 77 files changed, 322297 insertions(+), 253514 deletions(-)\n"," rename data/{ => data-old}/15000_pose_data_07_23_2023_GAUSS.csv (99%)\n"," rename data/{ => data-old}/15000_pose_data_07_23_2023_GAUSS_FORWARD.csv (100%)\n"," rename data/{ => data-old}/15000_pose_data_07_23_2023_GAUSS_INVERSE.csv (100%)\n"," rename data/{ => data-old}/15000_pose_data_07_25_2023.csv (99%)\n"," rename data/{ => data-old}/15000_pose_data_07_25_2023_FORWARD.csv (100%)\n"," rename data/{ => data-old}/15000_pose_data_07_25_2023_INVERSE.csv (100%)\n"," rename data/{ => data-old}/15000_pose_data_07_26_2023_FORWARD.csv (100%)\n"," rename data/{ => data-old}/15000_pose_data_07_26_2023_INVERSE.csv (100%)\n"," rename data/{ => data-old}/30000_pose_data_06_29_2023.csv (99%)\n"," rename data/{ => data-old}/30000_pose_data_07_05_2023.csv (99%)\n"," rename data/{ => data-old}/30000_pose_data_07_17_2023.csv (99%)\n"," rename data/{ => data-old}/30000_pose_data_07_20_2023.csv (99%)\n"," rename data/{ => data-old}/30000_pose_data_07_21_2023.csv (99%)\n"," rename data/{ => data-old}/30000_pose_data_07_21_2023_FORWARD.csv (100%)\n"," rename data/{ => data-old}/30000_pose_data_07_21_2023_INVERSE.csv (100%)\n"," rename data/{ => data-old}/30000_pose_data_07_21_2023_MOTOR_POS.csv (99%)\n"," rename data/{ => data-old}/30000_pose_raw_data_07_05_2023.csv (99%)\n"," rename data/{ => data-old}/compensation_data.csv (99%)\n"," rename data/{ => data-old}/inverse_7500_train_data.csv (99%)\n"," rename data/{ => data-old}/inverse_compensation_data.csv (99%)\n"," rename data/{ => data-old}/inverse_compensation_motor_data.csv (99%)\n"," rename data/{ => full}/15000_pose_data_07_26_2023.csv (99%)\n"," create mode 100644 data/repetability/repeatability_07_26_2023.csv\n"," create mode 100644 figures/repetability/pos_error.png\n"," create mode 100644 figures/repetability/pos_hist.png\n"," create mode 100644 figures/repetability/quat_error.png\n"," create mode 100644 figures/repetability/quat_hist.png\n"," create mode 100644 figures/workspace/z-x.png\n"," create mode 100644 matlab/DAQ Experiments/fifdeg.png\n"," create mode 100644 matlab/DAQ Experiments/fifdeg.txt\n"," create mode 100644 matlab/DAQ Experiments/fivedeg.png\n"," create mode 100644 matlab/DAQ Experiments/fivedeg.txt\n"," create mode 100644 matlab/DAQ Experiments/shear25mm.png\n"," create mode 100644 matlab/DAQ Experiments/shear25mm.txt\n"," create mode 100644 matlab/DAQ Experiments/tendeg.png\n"," create mode 100644 matlab/DAQ Experiments/tendeg.txt\n"," create mode 100644 matlab/DAQ Experiments/test.m\n"," create mode 100644 matlab/DAQ Experiments/twendeg.png\n"," create mode 100644 matlab/DAQ Experiments/twendeg.txt\n"," create mode 100644 matlab/DAQ Experiments/zerozero.png\n"," create mode 100644 matlab/DAQ Experiments/zerozero.txt\n"," create mode 100644 matlab/Tensile Experiments/minus13mm.txt\n"," create mode 100644 matlab/Tensile Experiments/minus6p5mm.txt\n"," create mode 100644 matlab/Tensile Experiments/plus13mm.txt\n"," create mode 100644 matlab/Tensile Experiments/plus22p5mm.txt\n"," create mode 100644 matlab/Tensile Experiments/plus6p5mm.txt\n"," create mode 100644 matlab/Tensile Experiments/zeromm.txt\n"," create mode 100644 matlab/brewermap.m\n"," create mode 100644 matlab/dL-tracking.svg\n"," create mode 100644 matlab/instron/convert-data.ipynb\n"," create mode 100644 matlab/instron/flex-shaft.csv\n"," create mode 100644 matlab/instron/flex-shaft.svg\n"," create mode 100644 matlab/instron/old/simple-azimuthal.csv\n"," create mode 100644 matlab/instron/old/simple-polar.csv\n"," create mode 100644 matlab/instron/old/truss-azimuthal.csv\n"," create mode 100644 matlab/instron/simple-printed-force.csv\n"," create mode 100644 matlab/instron/simple-printed-force.svg\n"," create mode 100644 matlab/instron/simple-printed-torque.svg\n"," create mode 100644 matlab/instron/simple-printed-torque_azimuth.csv\n"," create mode 100644 matlab/instron/simple-printed-torque_pole.csv\n"," create mode 100644 matlab/instron/simple-steel-force.csv\n"," create mode 100644 matlab/instron/simple-steel-force.svg\n"," create mode 100644 matlab/instron/simple-steel-torque.svg\n"," create mode 100644 matlab/instron/simple-steel-torque_azimuth.csv\n"," create mode 100644 matlab/instron/simple-steel-torque_pole.csv\n"," create mode 100644 matlab/instron/truss-steel-force.csv\n"," create mode 100644 matlab/instron/truss-steel-force.svg\n"," create mode 100644 matlab/instron/truss-steel-torque.svg\n"," create mode 100644 matlab/instron/truss-steel-torque_azimuth.csv\n"," create mode 100644 matlab/instron/truss-steel-torque_pole.csv\n"," create mode 100644 matlab/instron_plots.m\n"," create mode 100644 matlab/plot_cvjoint.m\n"," create mode 100644 matlab/repetability_analysis.m\n"," create mode 100644 matlab/theta-tracking.svg\n"," rename metrics/{forward_forward_2023_09_13-17_16_55_8.550mm => forward_2023_09_13-17_16_55_8.550mm} (100%)\n"," rename metrics/{forward_forward_2023_09_13-17_55_59_8.471mm => forward_2023_09_13-17_55_59_8.471mm} (100%)\n"," rename metrics/{forward_forward_2023_09_13-23_01_10_7.496mm => forward_2023_09_13-23_01_10_7_496mm.mat} (100%)\n"]}]},{"cell_type":"code","source":["%cd '/gdrive/My Drive/Github/aux-net'\n","!git add -A\n","!git commit -m \"Exporting raw test data\"\n","!git push"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rgvvpSe7ERVR","executionInfo":{"status":"ok","timestamp":1694701366951,"user_tz":240,"elapsed":124776,"user":{"displayName":"Kuba Kowalewski","userId":"08857268640727880536"}},"outputId":"c6c9e2e2-f248-4bb2-e5fe-12d132c77a14"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/My Drive/Github/aux-net\n","[main 90ac7f6] Training on cn and hn. Now exporting validation data.\n"," 2 files changed, 1 insertion(+), 1 deletion(-)\n"," delete mode 100644 models/inverse_2023_08_24-15_50_37_5.021pwm\n","Enumerating objects: 74, done.\n","Counting objects: 100% (74/74), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (70/70), done.\n","Writing objects: 100% (70/70), 581.31 MiB | 11.34 MiB/s, done.\n","Total 70 (delta 22), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (22/22), completed with 2 local objects.\u001b[K\n","remote: \u001b[1;31merror\u001b[m: Trace: db8df4a0b219ee2b84d543ae9e0b7d15fe425d47b279fd5398dfcdef16b4e561\u001b[K\n","remote: \u001b[1;31merror\u001b[m: See https://gh.io/lfs for more information.\u001b[K\n","remote: \u001b[1;31merror\u001b[m: File models/inverse_2023_08_22-14_09_21_5.022pwm is 127.26 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\n","remote: \u001b[1;31merror\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n","To https://github.com/KubaSrc/aux-net.git\n"," \u001b[31m! [remote rejected]\u001b[m main -> main (pre-receive hook declined)\n","\u001b[31merror: failed to push some refs to 'https://github.com/KubaSrc/aux-net.git'\n","\u001b[m"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1gWN38SWPROWxZxVnOYp14dk6VPKjPY6Q","timestamp":1689868361655}],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}